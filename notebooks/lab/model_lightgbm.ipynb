{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43cb608e",
   "metadata": {},
   "source": [
    "# LightGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623f8049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from lightgbm import LGBMRegressor # The ML model\n",
    "from utilsforecast.losses import rmse, mae\n",
    "from mlforecast import MLForecast\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "from hierarchicalforecast.core import HierarchicalReconciliation\n",
    "from hierarchicalforecast.methods import MinTrace, BottomUp\n",
    "from hierarchicalforecast.utils import aggregate\n",
    "from hierarchicalforecast.core import HierarchicalReconciliation\n",
    "\n",
    "from valuation.infra.store.dataset import DatasetStore\n",
    "from valuation.asset.identity.dataset import DatasetID\n",
    "from valuation.core.stage import DatasetStage\n",
    "from valuation.asset.identity.model import ModelPassport\n",
    "from valuation.asset.model.mlforecast import MLForecastModel\n",
    "from valuation.infra.store.model import ModelStore\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc1866f",
   "metadata": {},
   "source": [
    "## Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bcf1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ESTIMATORS = 100  # Number of trees for LightGBM\n",
    "NUM_CORES = 24\n",
    "SAFE_N_JOBS = max(1, NUM_CORES - 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35da959f",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ce279e",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = DatasetStore()\n",
    "dataset_id = DatasetID(name=\"train_val\", stage=DatasetStage.MODEL)\n",
    "passport = store.get_passport(dataset_id=dataset_id)\n",
    "ds = store.get(passport=passport)\n",
    "train_df = ds.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c54a5d",
   "metadata": {},
   "source": [
    "## Define the Model\n",
    "We instantate a LightGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb693aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = [LGBMRegressor(\n",
    "    random_state=42, \n",
    "    n_estimators=N_ESTIMATORS, # 100 trees is a good, fast start\n",
    "    n_jobs=SAFE_N_JOBS # Use the 'safe' n_jobs we defined\n",
    ")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e206bd5",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e68cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mf = MLForecast(\n",
    "    models=models,\n",
    "    freq=pd.offsets.Week(weekday=2), # Weekly frequency ending on Wednesday\n",
    "    # --- This is the automated feature engineering ---\n",
    "    lags=[52], # Use the value from 52 weeks ago as a feature\n",
    "    date_features=['week', 'month', 'year'] \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88113e58",
   "metadata": {},
   "source": [
    "## Blocked Cross-Validation\n",
    "This generates the unreconciled forecasts for each fold. We must add fitted=True to get the in-sample forecasts for the reconciler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be524319",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df_base = mf.cross_validation(\n",
    "    df=train_df,\n",
    "    h=52,\n",
    "    n_windows=5,\n",
    "    fitted=True\n",
    ")\n",
    "mf.fit(df=train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9337fa88",
   "metadata": {},
   "source": [
    "## Create and Persist the Model Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b04f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "passport = ModelPassport.create(\n",
    "    name=f\"lightgbm_model_{N_ESTIMATORS}_trees\",\n",
    "    description=f\"LightGBM model with {N_ESTIMATORS} trees and basic feature engineering\",\n",
    "    )\n",
    "model = MLForecastModel(passport=passport,model=mf)\n",
    "model_store = ModelStore()\n",
    "model_store.remove(passport=passport)\n",
    "model_store.add(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e858377",
   "metadata": {},
   "source": [
    "## Create Summing Matrix and Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9919aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Start with the core data (unique_id, ds, y)\n",
    "hierarchy_df = train_df[['unique_id', 'ds', 'y']].drop_duplicates() \n",
    "\n",
    "# 2. Create the grouping columns\n",
    "hierarchy_df['store'] = hierarchy_df['unique_id'].apply(lambda s: s.split('_')[0])\n",
    "hierarchy_df['category'] = hierarchy_df['unique_id'].apply(lambda s: s.split('_')[1])\n",
    "\n",
    "# 3. Drop the 'unique_id' column from the input DF before aggregation\n",
    "#    The aggregate function knows to use the combination of columns in 'spec'\n",
    "#    to uniquely identify the time series levels.\n",
    "hierarchy_df_clean = hierarchy_df.drop(columns=['unique_id']) # ðŸ‘ˆ ADD THIS LINE\n",
    "\n",
    "spec = [['store'], ['category'], ['store', 'category']]\n",
    "\n",
    "# Pass the cleaned DataFrame to the aggregate function\n",
    "_, S_df, tags = aggregate(df=hierarchy_df_clean, spec=spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24b160d",
   "metadata": {},
   "source": [
    "## Aggregate Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c822b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and Prepare Y_hat_df_base\n",
    "Y_hat_df_base = cv_df_base.drop(columns=['cutoff', 'y'], errors='ignore')\n",
    "\n",
    "# 1. Add the hierarchy columns to the base forecasts\n",
    "Y_hat_df_base_clean = Y_hat_df_base.copy()\n",
    "Y_hat_df_base_clean['store'] = Y_hat_df_base_clean['unique_id'].apply(lambda s: s.split('_')[0])\n",
    "Y_hat_df_base_clean['category'] = Y_hat_df_base_clean['unique_id'].apply(lambda s: s.split('_')[1])\n",
    "\n",
    "# 2. Identify the forecast column(s) - typically 'LGBMRegressor' or similar model name\n",
    "forecast_col = 'LGBMRegressor'  # Adjust if your column has a different name\n",
    "\n",
    "# 3. Rename forecast column to 'y' temporarily for aggregation\n",
    "Y_hat_df_base_for_agg = Y_hat_df_base_clean.copy()\n",
    "Y_hat_df_base_for_agg = Y_hat_df_base_for_agg.rename(columns={forecast_col: 'y'})\n",
    "Y_hat_df_base_for_agg = Y_hat_df_base_for_agg.drop(columns=['unique_id'])\n",
    "\n",
    "# 4. Aggregate to create forecasts at all hierarchy levels\n",
    "Y_hat_aggregated, _, _ = aggregate(df=Y_hat_df_base_for_agg, spec=spec)\n",
    "\n",
    "# 5. Rename back to original forecast column name\n",
    "Y_hat_aggregated = Y_hat_aggregated.rename(columns={'y': forecast_col})\n",
    "\n",
    "print(\"Forecasts aggregated successfully across all hierarchy levels.\")\n",
    "print(f\"Base forecasts shape: {Y_hat_df_base.shape}\")\n",
    "print(f\"Aggregated forecasts shape: {Y_hat_aggregated.shape}\")\n",
    "print(f\"Unique IDs in aggregated forecasts: {Y_hat_aggregated['unique_id'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bc6e33",
   "metadata": {},
   "source": [
    "## Reconciler for CV Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfff44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconcilers = [MinTrace(method='ols')]\n",
    "hrec = HierarchicalReconciliation(reconcilers=reconcilers)\n",
    "\n",
    "# Prepare aggregated actuals\n",
    "Y_df_base = cv_df_base[['unique_id', 'ds', 'y']].copy()\n",
    "Y_df_base['store'] = Y_df_base['unique_id'].apply(lambda s: s.split('_')[0])\n",
    "Y_df_base['category'] = Y_df_base['unique_id'].apply(lambda s: s.split('_')[1])\n",
    "Y_df_base_for_agg = Y_df_base.drop(columns=['unique_id'])\n",
    "Y_df_actuals, _, _ = aggregate(df=Y_df_base_for_agg, spec=spec)\n",
    "\n",
    "# Reconcile (this adjusts the aggregated forecasts for coherence)\n",
    "cv_df_reconciled = hrec.reconcile(\n",
    "    Y_hat_df=Y_hat_aggregated,  # Now has all hierarchy levels\n",
    "    Y_df=Y_df_actuals,\n",
    "    S_df=S_df,\n",
    "    tags=tags\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f42215",
   "metadata": {},
   "source": [
    "## Evaluate CV Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad9ba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "\n",
    "# 1. Get actuals at ALL hierarchy levels\n",
    "actuals_base = cv_df_base[['unique_id', 'ds', 'cutoff', 'y']].copy()\n",
    "actuals_base['store'] = actuals_base['unique_id'].apply(lambda s: s.split('_')[0])\n",
    "actuals_base['category'] = actuals_base['unique_id'].apply(lambda s: s.split('_')[1])\n",
    "actuals_base_for_agg = actuals_base.drop(columns=['unique_id'])\n",
    "\n",
    "# Aggregate actuals\n",
    "actuals_aggregated, _, _ = aggregate(df=actuals_base_for_agg, spec=spec)\n",
    "\n",
    "# 2. Merge forecasts with actuals\n",
    "cv_df_eval = cv_df_reconciled.merge(\n",
    "    actuals_aggregated[['unique_id', 'ds', 'y']], \n",
    "    on=['unique_id', 'ds'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 3. Classify hierarchy levels properly\n",
    "def classify_level(uid):\n",
    "    if '_' in uid:\n",
    "        return 'bottom'  # store_category (e.g., \"100_beer\")\n",
    "    elif '/' in uid:\n",
    "        return 'store_category'  # aggregated store/category (e.g., \"100/beer\")\n",
    "    else:\n",
    "        # Check if it's a store (numeric) or category (text)\n",
    "        try:\n",
    "            int(uid)\n",
    "            return 'store'  # Just store (e.g., \"100\")\n",
    "        except:\n",
    "            return 'category'  # Just category (e.g., \"beer\")\n",
    "\n",
    "cv_df_eval['level'] = cv_df_eval['unique_id'].apply(classify_level)\n",
    "\n",
    "# 4. Get model columns\n",
    "model_cols = [col for col in cv_df_reconciled.columns \n",
    "              if col not in ['unique_id', 'ds', 'cutoff']]\n",
    "\n",
    "print(f\"Found model columns: {model_cols}\")\n",
    "print(f\"\\nDataset overview:\")\n",
    "print(f\"  Total forecasts: {len(cv_df_eval):,}\")\n",
    "print(f\"  Unique series: {cv_df_eval['unique_id'].nunique():,}\")\n",
    "print(f\"  Date range: {cv_df_eval['ds'].min()} to {cv_df_eval['ds'].max()}\")\n",
    "print(f\"\\nActual values summary:\")\n",
    "print(cv_df_eval['y'].describe())\n",
    "\n",
    "# 5. Overall Performance\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OVERALL PERFORMANCE (All Hierarchy Levels)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "performance_results = []\n",
    "for model_col in model_cols:\n",
    "    mask = cv_df_eval[[model_col, 'y']].notna().all(axis=1)\n",
    "    y_true = cv_df_eval.loc[mask, 'y']\n",
    "    y_pred = cv_df_eval.loc[mask, model_col]\n",
    "    \n",
    "    performance_results.append({\n",
    "        'model': model_col.replace('LGBMRegressor/', ''),  # Shorter names\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        'MAE': mean_absolute_error(y_true, y_pred),\n",
    "        'MAPE': mean_absolute_percentage_error(y_true, y_pred) * 100,  # As percentage\n",
    "        'Mean_Actual': y_true.mean(),\n",
    "        'n_forecasts': len(y_true)\n",
    "    })\n",
    "\n",
    "overall_perf = pd.DataFrame(performance_results).set_index('model')\n",
    "print(overall_perf.to_string())\n",
    "\n",
    "# 6. Performance by Hierarchy Level\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERFORMANCE BY HIERARCHY LEVEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "levels_order = ['bottom', 'store_category', 'store', 'category']\n",
    "level_results = []\n",
    "\n",
    "for level in levels_order:\n",
    "    level_data = cv_df_eval[cv_df_eval['level'] == level]\n",
    "    \n",
    "    if len(level_data) == 0:\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\n{level.upper()} Level:\")\n",
    "    print(f\"  Unique series: {level_data['unique_id'].nunique():,}\")\n",
    "    print(f\"  Total forecasts: {len(level_data):,}\")\n",
    "    print(f\"  Actual mean: {level_data['y'].mean():.2f}, std: {level_data['y'].std():.2f}\")\n",
    "    \n",
    "    for model_col in model_cols:\n",
    "        mask = level_data[[model_col, 'y']].notna().all(axis=1)\n",
    "        y_true = level_data.loc[mask, 'y']\n",
    "        y_pred = level_data.loc[mask, model_col]\n",
    "        \n",
    "        if len(y_true) > 0:\n",
    "            rmse_val = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "            mae_val = mean_absolute_error(y_true, y_pred)\n",
    "            mape_val = mean_absolute_percentage_error(y_true, y_pred) * 100\n",
    "            \n",
    "            level_results.append({\n",
    "                'Level': level,\n",
    "                'Model': model_col.replace('LGBMRegressor/', ''),\n",
    "                'RMSE': rmse_val,\n",
    "                'MAE': mae_val,\n",
    "                'MAPE%': mape_val,\n",
    "                'Mean_Actual': y_true.mean(),\n",
    "                'n': len(y_true)\n",
    "            })\n",
    "            \n",
    "            # Normalized error (MAE as % of mean)\n",
    "            normalized_mae = (mae_val / y_true.mean() * 100) if y_true.mean() > 0 else 0\n",
    "            \n",
    "            print(f\"    {model_col.replace('LGBMRegressor/', '')[:30]:30s} -> \"\n",
    "                  f\"RMSE: {rmse_val:>10.2f}, MAE: {mae_val:>10.2f}, \"\n",
    "                  f\"MAPE: {mape_val:>6.2f}%, Norm_MAE: {normalized_mae:>6.2f}%\")\n",
    "\n",
    "# 7. Comparison Table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECONCILIATION IMPACT (Comparing Base vs Reconciled)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "level_perf_df = pd.DataFrame(level_results)\n",
    "if len(level_perf_df) > 0:\n",
    "    comparison = level_perf_df.pivot_table(\n",
    "        index='Level',\n",
    "        columns='Model',\n",
    "        values=['RMSE', 'MAE', 'MAPE%']\n",
    "    )\n",
    "    print(comparison.to_string())\n",
    "\n",
    "# 8. Sample predictions vs actuals\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE PREDICTIONS (First 10 bottom-level forecasts)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "sample = cv_df_eval[cv_df_eval['level'] == 'bottom'].head(10)[\n",
    "    ['unique_id', 'ds', 'y'] + model_cols\n",
    "].round(2)\n",
    "print(sample.to_string())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "valuation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
