{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43cb608e",
   "metadata": {},
   "source": [
    "# LightGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623f8049",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor # The ML model\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import wandb\n",
    "\n",
    "from hierarchicalforecast.core import HierarchicalReconciliation\n",
    "from hierarchicalforecast.methods import MinTrace, BottomUp\n",
    "from hierarchicalforecast.utils import aggregate\n",
    "from hierarchicalforecast.core import HierarchicalReconciliation\n",
    "from mlforecast import MLForecast\n",
    "\n",
    "from valuation.infra.store.dataset import DatasetStore\n",
    "from valuation.asset.identity.dataset import DatasetID\n",
    "from valuation.core.stage import DatasetStage\n",
    "from valuation.asset.identity.model import ModelPassport\n",
    "from valuation.asset.model.mlforecast import MLForecastModel\n",
    "from valuation.infra.store.model import ModelStore\n",
    "from valuation.flow.modeling.model_selection.light_gbm import LightGBMHP\n",
    "from valuation.flow.modeling.model_selection.cv import CrossValidationHP\n",
    "from valuation.flow.modeling.model_selection.mlforecast import MLForecastHP\n",
    "from valuation.flow.modeling.model_selection.base import ModelParams\n",
    "# Suppress the specific warning\n",
    "warnings.filterwarnings('ignore', message='.*lag.*')\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc1866f",
   "metadata": {},
   "source": [
    "## Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bcf1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CORES = 24              # Total CPU cores available\n",
    "# Reproducibility \n",
    "RANDOM_STATE = 42           # Fixed random seed\n",
    "# LightGBM model parameters\n",
    "lightgbm_params = LightGBMHP()      # See the dataclass for details on defaults\n",
    "mlforecast_params = MLForecastHP()  # See the dataclass for details on defaults\n",
    "cross_validation_params = CrossValidationHP()  # See the dataclass for details on defaults\n",
    "model_params = ModelParams(\n",
    "    light_gbm=lightgbm_params,\n",
    "    mlforecast=mlforecast_params,\n",
    "    cross_validation=cross_validation_params\n",
    ")\n",
    "print(model_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8b4d57",
   "metadata": {},
   "source": [
    "## Register the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d965207",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"valuation\", job_type=\"modeling\")\n",
    "run.config.update(lightgbm_params.as_dict())\n",
    "run.config.update(mlforecast_params.as_dict())\n",
    "run.config.update(cross_validation_params.as_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35da959f",
   "metadata": {},
   "source": [
    "## Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ce279e",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = DatasetStore()\n",
    "dataset_id = DatasetID(name=\"train_val\", stage=DatasetStage.MODEL)\n",
    "passport = store.get_passport(dataset_id=dataset_id)\n",
    "ds = store.get(passport=passport)\n",
    "train_df = ds.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c54a5d",
   "metadata": {},
   "source": [
    "## Define the Model\n",
    "We instantate a LightGBM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb693aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = [LGBMRegressor(**lightgbm_params.as_dict())]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab67fc5",
   "metadata": {},
   "source": [
    "## Set Cross Validation Lag Parameters from Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e206bd5",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e68cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mf = MLForecast(\n",
    "    models=models,**mlforecast_params.as_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88113e58",
   "metadata": {},
   "source": [
    "## Blocked Cross-Validation\n",
    "This generates the unreconciled forecasts for each fold. We must add fitted=True to get the in-sample forecasts for the reconciler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be524319",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_df_base = mf.cross_validation(\n",
    "    df=train_df, **cross_validation_params.as_dict())\n",
    "mf.fit(df=train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9337fa88",
   "metadata": {},
   "source": [
    "## Create and Persist the Model Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b04f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "passport = ModelPassport.create(\n",
    "    name=f\"lightgbm_model_{lightgbm_params.n_estimators}_trees\",\n",
    "    description=f\"LightGBM model with {lightgbm_params.n_estimators} trees and basic feature engineering\",\n",
    "    )\n",
    "model = MLForecastModel(passport=passport,model=mf)\n",
    "model_store = ModelStore()\n",
    "model_store.remove(passport=passport)\n",
    "model_store.add(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e858377",
   "metadata": {},
   "source": [
    "## Create Summing Matrix and Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9919aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Start with the core data (unique_id, ds, y)\n",
    "hierarchy_df = train_df[['unique_id', 'ds', 'y']].drop_duplicates() \n",
    "\n",
    "# 2. Create the grouping columns\n",
    "hierarchy_df['store'] = hierarchy_df['unique_id'].apply(lambda s: s.split('_')[0])\n",
    "hierarchy_df['category'] = hierarchy_df['unique_id'].apply(lambda s: s.split('_')[1])\n",
    "\n",
    "# 3. Drop the 'unique_id' column from the input DF before aggregation\n",
    "#    The aggregate function knows to use the combination of columns in 'spec'\n",
    "#    to uniquely identify the time series levels.\n",
    "hierarchy_df_clean = hierarchy_df.drop(columns=['unique_id']) # üëà ADD THIS LINE\n",
    "\n",
    "spec = [['store'], ['category'], ['store', 'category']]\n",
    "\n",
    "# Pass the cleaned DataFrame to the aggregate function\n",
    "_, S_df, tags = aggregate(df=hierarchy_df_clean, spec=spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24b160d",
   "metadata": {},
   "source": [
    "## Aggregate Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c822b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean and Prepare Y_hat_df_base\n",
    "Y_hat_df_base = cv_df_base.drop(columns=['cutoff', 'y'], errors='ignore')\n",
    "\n",
    "# 1. Add the hierarchy columns to the base forecasts\n",
    "Y_hat_df_base_clean = Y_hat_df_base.copy()\n",
    "Y_hat_df_base_clean['store'] = Y_hat_df_base_clean['unique_id'].apply(lambda s: s.split('_')[0])\n",
    "Y_hat_df_base_clean['category'] = Y_hat_df_base_clean['unique_id'].apply(lambda s: s.split('_')[1])\n",
    "\n",
    "# 2. Identify the forecast column(s) - typically 'LGBMRegressor' or similar model name\n",
    "forecast_col = 'LGBMRegressor'  # Adjust if your column has a different name\n",
    "\n",
    "# 3. Rename forecast column to 'y' temporarily for aggregation\n",
    "Y_hat_df_base_for_agg = Y_hat_df_base_clean.copy()\n",
    "Y_hat_df_base_for_agg = Y_hat_df_base_for_agg.rename(columns={forecast_col: 'y'})\n",
    "Y_hat_df_base_for_agg = Y_hat_df_base_for_agg.drop(columns=['unique_id'])\n",
    "\n",
    "# 4. Aggregate to create forecasts at all hierarchy levels\n",
    "Y_hat_aggregated, _, _ = aggregate(df=Y_hat_df_base_for_agg, spec=spec)\n",
    "\n",
    "# 5. Rename back to original forecast column name\n",
    "Y_hat_aggregated = Y_hat_aggregated.rename(columns={'y': forecast_col})\n",
    "\n",
    "print(\"Forecasts aggregated successfully across all hierarchy levels.\")\n",
    "print(f\"Base forecasts shape: {Y_hat_df_base.shape}\")\n",
    "print(f\"Aggregated forecasts shape: {Y_hat_aggregated.shape}\")\n",
    "print(f\"Unique IDs in aggregated forecasts: {Y_hat_aggregated['unique_id'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bc6e33",
   "metadata": {},
   "source": [
    "## Reconciler for CV Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfff44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "reconcilers = [MinTrace(method='ols')]\n",
    "hrec = HierarchicalReconciliation(reconcilers=reconcilers)\n",
    "\n",
    "# Prepare aggregated actuals\n",
    "Y_df_base = cv_df_base[['unique_id', 'ds', 'y']].copy()\n",
    "Y_df_base['store'] = Y_df_base['unique_id'].apply(lambda s: s.split('_')[0])\n",
    "Y_df_base['category'] = Y_df_base['unique_id'].apply(lambda s: s.split('_')[1])\n",
    "Y_df_base_for_agg = Y_df_base.drop(columns=['unique_id'])\n",
    "Y_df_actuals, _, _ = aggregate(df=Y_df_base_for_agg, spec=spec)\n",
    "\n",
    "# Reconcile (this adjusts the aggregated forecasts for coherence)\n",
    "cv_df_reconciled = hrec.reconcile(\n",
    "    Y_hat_df=Y_hat_aggregated,  # Now has all hierarchy levels\n",
    "    Y_df=Y_df_actuals,\n",
    "    S_df=S_df,\n",
    "    tags=tags\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f42215",
   "metadata": {},
   "source": [
    "## Evaluate CV Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad9ba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "\n",
    "# 1. Get actuals at ALL hierarchy levels\n",
    "actuals_base = cv_df_base[['unique_id', 'ds', 'cutoff', 'y']].copy()\n",
    "actuals_base['store'] = actuals_base['unique_id'].apply(lambda s: s.split('_')[0])\n",
    "actuals_base['category'] = actuals_base['unique_id'].apply(lambda s: s.split('_')[1])\n",
    "actuals_base_for_agg = actuals_base.drop(columns=['unique_id'])\n",
    "\n",
    "# Aggregate actuals\n",
    "actuals_aggregated, _, _ = aggregate(df=actuals_base_for_agg, spec=spec)\n",
    "\n",
    "# 2. Merge forecasts with actuals\n",
    "cv_df_eval = cv_df_reconciled.merge(\n",
    "    actuals_aggregated[['unique_id', 'ds', 'y']], \n",
    "    on=['unique_id', 'ds'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# 3. Classify hierarchy levels properly\n",
    "def classify_level(uid):\n",
    "    if '_' in uid:\n",
    "        return 'bottom'  # store_category (e.g., \"100_beer\")\n",
    "    elif '/' in uid:\n",
    "        return 'store_category'  # aggregated store/category (e.g., \"100/beer\")\n",
    "    else:\n",
    "        # Check if it's a store (numeric) or category (text)\n",
    "        try:\n",
    "            int(uid)\n",
    "            return 'store'  # Just store (e.g., \"100\")\n",
    "        except:\n",
    "            return 'category'  # Just category (e.g., \"beer\")\n",
    "\n",
    "cv_df_eval['level'] = cv_df_eval['unique_id'].apply(classify_level)\n",
    "\n",
    "# 4. Get model columns\n",
    "model_cols = [col for col in cv_df_reconciled.columns \n",
    "              if col not in ['unique_id', 'ds', 'cutoff']]\n",
    "\n",
    "print(f\"Found model columns: {model_cols}\")\n",
    "print(f\"\\nDataset overview:\")\n",
    "print(f\"  Total forecasts: {len(cv_df_eval):,}\")\n",
    "print(f\"  Unique series: {cv_df_eval['unique_id'].nunique():,}\")\n",
    "print(f\"  Date range: {cv_df_eval['ds'].min()} to {cv_df_eval['ds'].max()}\")\n",
    "print(f\"\\nActual values summary:\")\n",
    "print(cv_df_eval['y'].describe())\n",
    "\n",
    "# 5. Overall Performance\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OVERALL PERFORMANCE (All Hierarchy Levels)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "performance_results = []\n",
    "for model_col in model_cols:\n",
    "    mask = cv_df_eval[[model_col, 'y']].notna().all(axis=1)\n",
    "    y_true = cv_df_eval.loc[mask, 'y']\n",
    "    y_pred = cv_df_eval.loc[mask, model_col]\n",
    "    \n",
    "    performance = {\n",
    "        'model': model_col.replace('LGBMRegressor/', ''),  # Shorter names\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        'MAE': mean_absolute_error(y_true, y_pred),\n",
    "        'MAPE': mean_absolute_percentage_error(y_true, y_pred) * 100,  # As percentage\n",
    "        'Mean_Actual': y_true.mean(),\n",
    "        'n_forecasts': len(y_true)}\n",
    "    \n",
    "    performance_results.append(performance)\n",
    "    run.log(performance)\n",
    "\n",
    "overall_perf = pd.DataFrame(performance_results).set_index('model')\n",
    "print(overall_perf.to_string())\n",
    "\n",
    "# 6. Performance by Hierarchy Level\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PERFORMANCE BY HIERARCHY LEVEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "levels_order = ['bottom', 'store_category', 'store', 'category']\n",
    "level_results = []\n",
    "\n",
    "for level in levels_order:\n",
    "    level_data = cv_df_eval[cv_df_eval['level'] == level]\n",
    "    \n",
    "    if len(level_data) == 0:\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\n{level.upper()} Level:\")\n",
    "    print(f\"  Unique series: {level_data['unique_id'].nunique():,}\")\n",
    "    print(f\"  Total forecasts: {len(level_data):,}\")\n",
    "    print(f\"  Actual mean: {level_data['y'].mean():.2f}, std: {level_data['y'].std():.2f}\")\n",
    "    \n",
    "    for model_col in model_cols:\n",
    "        mask = level_data[[model_col, 'y']].notna().all(axis=1)\n",
    "        y_true = level_data.loc[mask, 'y']\n",
    "        y_pred = level_data.loc[mask, model_col]\n",
    "        \n",
    "        if len(y_true) > 0:\n",
    "            rmse_val = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "            mae_val = mean_absolute_error(y_true, y_pred)\n",
    "            mape_val = mean_absolute_percentage_error(y_true, y_pred) * 100\n",
    "            \n",
    "            level_results.append({\n",
    "                'Level': level,\n",
    "                'Model': model_col.replace('LGBMRegressor/', ''),\n",
    "                'RMSE': rmse_val,\n",
    "                'MAE': mae_val,\n",
    "                'MAPE%': mape_val,\n",
    "                'Mean_Actual': y_true.mean(),\n",
    "                'n': len(y_true)\n",
    "            })\n",
    "            \n",
    "            # Normalized error (MAE as % of mean)\n",
    "            normalized_mae = (mae_val / y_true.mean() * 100) if y_true.mean() > 0 else 0\n",
    "            \n",
    "            print(f\"    {model_col.replace('LGBMRegressor/', '')[:30]:30s} -> \"\n",
    "                  f\"RMSE: {rmse_val:>10.2f}, MAE: {mae_val:>10.2f}, \"\n",
    "                  f\"MAPE: {mape_val:>6.2f}%, Norm_MAE: {normalized_mae:>6.2f}%\")\n",
    "\n",
    "# 7. Comparison Table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECONCILIATION IMPACT (Comparing Base vs Reconciled)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "level_perf_df = pd.DataFrame(level_results)\n",
    "if len(level_perf_df) > 0:\n",
    "    comparison = level_perf_df.pivot_table(\n",
    "        index='Level',\n",
    "        columns='Model',\n",
    "        values=['RMSE', 'MAE', 'MAPE%']\n",
    "    )\n",
    "    print(comparison.to_string())\n",
    "\n",
    "# 8. Sample predictions vs actuals\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE PREDICTIONS (First 10 bottom-level forecasts)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "sample = cv_df_eval[cv_df_eval['level'] == 'bottom'].head(10)[\n",
    "    ['unique_id', 'ds', 'y'] + model_cols\n",
    "].round(2)\n",
    "print(sample.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f6656b",
   "metadata": {},
   "source": [
    "## Model Performance Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295fa077",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PERFORMANCE DIAGNOSTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 1. Check imputation impact\n",
    "print(\"\\n1. IMPUTATION ANALYSIS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Check what percentage of data was originally missing\n",
    "# This assumes train_df is your final densified/imputed dataset\n",
    "if 'train_df' in locals():\n",
    "    # Check series lengths - all should be equal after densification\n",
    "    series_lengths = train_df.groupby('unique_id').size()\n",
    "    print(f\"Series lengths after densification:\")\n",
    "    print(f\"  Min: {series_lengths.min()}, Max: {series_lengths.max()}\")\n",
    "    print(f\"  All equal: {len(series_lengths.unique()) == 1}\")\n",
    "    \n",
    "    # If you tracked NaNs before imputation, report it\n",
    "    # Otherwise, skip this section\n",
    "    print(\"\\n‚ö†Ô∏è  Note: Imputation percentage not tracked.\")\n",
    "    print(\"   If performance is poor, imputation may be the cause.\")\n",
    "\n",
    "# 2. Check training data quality\n",
    "print(\"\\n2. TRAINING DATA QUALITY\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "if 'train_df' in locals():\n",
    "    print(f\"Training data shape: {train_df.shape}\")\n",
    "    print(f\"Unique series: {train_df['unique_id'].nunique()}\")\n",
    "    print(f\"Date range: {train_df['ds'].min()} to {train_df['ds'].max()}\")\n",
    "    \n",
    "    # Check for zero/near-zero values\n",
    "    zero_pct = (train_df['y'] == 0).sum() / len(train_df) * 100\n",
    "    near_zero_pct = (train_df['y'] < 1).sum() / len(train_df) * 100\n",
    "    print(f\"\\nZero values: {zero_pct:.1f}%\")\n",
    "    print(f\"Near-zero (<1): {near_zero_pct:.1f}%\")\n",
    "    \n",
    "    # Revenue distribution\n",
    "    print(f\"\\nRevenue distribution:\")\n",
    "    print(train_df['y'].describe())\n",
    "    \n",
    "    # Check for negative values\n",
    "    if (train_df['y'] < 0).any():\n",
    "        print(f\"‚ö†Ô∏è  WARNING: {(train_df['y'] < 0).sum()} negative values found!\")\n",
    "\n",
    "# 3. Check predictions quality\n",
    "print(\"\\n3. PREDICTION QUALITY ANALYSIS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "if 'cv_df_eval' in locals() and 'model_cols' in locals():\n",
    "    # Check for extreme predictions\n",
    "    for model_col in model_cols:\n",
    "        preds = cv_df_eval[model_col].dropna()\n",
    "        actuals = cv_df_eval['y'].dropna()\n",
    "        \n",
    "        print(f\"\\n{model_col}:\")\n",
    "        print(f\"  Predictions range: [{preds.min():.2f}, {preds.max():.2f}]\")\n",
    "        print(f\"  Actuals range: [{actuals.min():.2f}, {actuals.max():.2f}]\")\n",
    "        print(f\"  Predictions mean: {preds.mean():.2f} vs Actuals mean: {actuals.mean():.2f}\")\n",
    "        \n",
    "        # Check for negative predictions\n",
    "        neg_preds = (preds < 0).sum()\n",
    "        if neg_preds > 0:\n",
    "            print(f\"  ‚ö†Ô∏è  {neg_preds} negative predictions ({neg_preds/len(preds)*100:.1f}%)\")\n",
    "        \n",
    "        # Check for extreme predictions\n",
    "        extreme_high = (preds > actuals.quantile(0.99) * 2).sum()\n",
    "        if extreme_high > 0:\n",
    "            print(f\"  ‚ö†Ô∏è  {extreme_high} extremely high predictions\")\n",
    "\n",
    "# 4. Residual analysis\n",
    "print(\"\\n4. RESIDUAL ANALYSIS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "if 'cv_df_eval' in locals() and len(model_cols) > 0:\n",
    "    model_col = model_cols[0]  # Analyze first model\n",
    "    \n",
    "    mask = cv_df_eval[[model_col, 'y']].notna().all(axis=1)\n",
    "    actuals = cv_df_eval.loc[mask, 'y']\n",
    "    preds = cv_df_eval.loc[mask, model_col]\n",
    "    residuals = actuals - preds\n",
    "    \n",
    "    print(f\"Analyzing {model_col}:\")\n",
    "    print(f\"  Mean residual: {residuals.mean():.2f}\")\n",
    "    print(f\"  Median residual: {residuals.median():.2f}\")\n",
    "    print(f\"  Residual std: {residuals.std():.2f}\")\n",
    "    print(f\"  Mean absolute error: {np.abs(residuals).mean():.2f}\")\n",
    "    \n",
    "    # Check for systematic bias\n",
    "    if abs(residuals.mean()) > actuals.std() * 0.1:\n",
    "        print(f\"  ‚ö†Ô∏è  WARNING: Systematic bias detected!\")\n",
    "        if residuals.mean() > 0:\n",
    "            print(f\"     Model is UNDER-predicting on average\")\n",
    "        else:\n",
    "            print(f\"     Model is OVER-predicting on average\")\n",
    "\n",
    "# 5. Check differences transformation\n",
    "print(\"\\n5. DIFFERENCING IMPACT\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "if 'TARGET_TRANSFORMS' in locals() and len(mlforecast_params.target_transforms) > 0:\n",
    "    print(f\"Target transforms applied: {mlforecast_params.target_transforms}\")\n",
    "    print(\"‚ö†Ô∏è  Differencing can cause issues if:\")\n",
    "    print(\"   - Series are short\")\n",
    "    print(\"   - Series have many imputed values\")\n",
    "    print(\"   - Series are already stationary\")\n",
    "    print(\"\\nüí° RECOMMENDATION: Try removing Differences([1]) and see if performance improves\")\n",
    "\n",
    "# 6. Check for data leakage\n",
    "print(\"\\n6. DATA LEAKAGE CHECK\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "if 'cv_df_base' in locals():\n",
    "    # Check if CV is working correctly\n",
    "    cv_windows = cv_df_base.groupby(['unique_id', 'cutoff']).size()\n",
    "    print(f\"CV windows per series: {cv_windows.groupby(level=0).size().value_counts()}\")\n",
    "    \n",
    "    # Check cutoff dates\n",
    "    print(f\"\\nCutoff dates: {sorted(cv_df_base['cutoff'].unique())}\")\n",
    "\n",
    "# 7. Specific problematic series\n",
    "print(\"\\n7. WORST PERFORMING SERIES\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "if 'cv_df_eval' in locals() and 'model_cols' in locals():\n",
    "    model_col = model_cols[0]\n",
    "    \n",
    "    # Calculate MAE per series\n",
    "    series_mae = cv_df_eval.groupby('unique_id').apply(\n",
    "        lambda x: mean_absolute_error(x['y'].dropna(), x[model_col].dropna()) \n",
    "        if len(x['y'].dropna()) > 0 else np.nan\n",
    "    ).sort_values(ascending=False)\n",
    "    \n",
    "    print(\"Top 10 worst series (by MAE):\")\n",
    "    for uid, mae in series_mae.head(10).items():\n",
    "        series_data = cv_df_eval[cv_df_eval['unique_id'] == uid]\n",
    "        actual_mean = series_data['y'].mean()\n",
    "        pred_mean = series_data[model_col].mean()\n",
    "        print(f\"  {uid:30s} MAE: {mae:>10.2f}, Actual mean: {actual_mean:>10.2f}, Pred mean: {pred_mean:>10.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\"\"\n",
    "Based on diagnostics above, try these fixes:\n",
    "\n",
    "1. REMOVE DIFFERENCING if series are short or heavily imputed:\n",
    "   TARGET_TRANSFORMS = []  # Remove Differences([1])\n",
    "\n",
    "2. REDUCE LAGS if many series are short:\n",
    "   LAGS = [1, 2, 4, 8, 13]  # Remove 26, 52\n",
    "\n",
    "3. FILTER OUT heavily imputed series (>30% imputed)\n",
    "\n",
    "4. CHECK if forward/backward fill is creating unrealistic patterns\n",
    "\n",
    "5. INCREASE min_child_samples for more regularization:\n",
    "   min_child_samples=50 or 100\n",
    "\n",
    "6. ADD more regularization:\n",
    "   reg_alpha=1.0, reg_lambda=1.0\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "valuation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
