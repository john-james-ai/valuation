{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31763d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from valuation.asset.identity.dataset import DatasetID\n",
    "from valuation.core.stage import DatasetStage\n",
    "from valuation.infra.store.dataset import DatasetStore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f12291",
   "metadata": {},
   "source": [
    "## Get Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6135af6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-10-24 01:05:59.743\u001b[0m | \u001b[34m\u001b[1mDEBUG   \u001b[0m | \u001b[36mvaluation.asset.dataset.base\u001b[0m:\u001b[36mload\u001b[0m:\u001b[36m338\u001b[0m - \u001b[34m\u001b[1mDataset Dataset train_val of the model stage created on 2025-10-24 at 00:50 loaded.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "store = DatasetStore()\n",
    "dataset_id = DatasetID(name=\"train_val\", stage=DatasetStage.MODEL)\n",
    "passport = store.get_passport(dataset_id=dataset_id)\n",
    "ds = store.get(passport=passport)\n",
    "train_df = ds.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bab399",
   "metadata": {},
   "source": [
    "## Check Date Differences and Number of Periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e70117d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency check for a sample series:\n",
      "ds\n",
      "7 days    312\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Expected number of weeks in train_df (approx): 313\n",
      "\n",
      "Checking completeness per series:\n",
      "                            min        max  count\n",
      "unique_id                                        \n",
      "100_analgesics       1990-01-03 1995-12-27    313\n",
      "100_bath soap        1990-01-03 1995-12-27    313\n",
      "100_bathroom tissues 1990-01-03 1995-12-27    313\n",
      "100_beer             1990-01-03 1995-12-27    313\n",
      "100_bottled juices   1990-01-03 1995-12-27    313\n",
      "\n",
      "All series appear to have the expected number of periods.\n"
     ]
    }
   ],
   "source": [
    "# Check frequency of the 'ds' column in your train_df\n",
    "# Calculate the difference between consecutive dates for a sample series\n",
    "sample_id = train_df['unique_id'].iloc[0]\n",
    "sample_series_dates = train_df[train_df['unique_id'] == sample_id]['ds'].sort_values()\n",
    "date_diffs = sample_series_dates.diff().dropna()\n",
    "\n",
    "print(\"Frequency check for a sample series:\")\n",
    "print(date_diffs.value_counts())\n",
    "\n",
    "# Check for missing periods per series\n",
    "completeness_check = train_df.groupby('unique_id')['ds'].agg(['min', 'max', 'count'])\n",
    "expected_weeks_train = (train_df['ds'].max() - train_df['ds'].min()).days // 7 + 1 # Approximate for 6 years\n",
    "\n",
    "print(f\"\\nExpected number of weeks in train_df (approx): {expected_weeks_train}\")\n",
    "print(\"\\nChecking completeness per series:\")\n",
    "print(completeness_check.head())\n",
    "\n",
    "# Find series that don't have the expected number of weeks\n",
    "incomplete_series = completeness_check[completeness_check['count'] != expected_weeks_train]\n",
    "if not incomplete_series.empty:\n",
    "    print(f\"\\nFound {len(incomplete_series)} series with potentially missing periods:\")\n",
    "    print(incomplete_series.head())\n",
    "else:\n",
    "    print(\"\\nAll series appear to have the expected number of periods.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92261c0",
   "metadata": {},
   "source": [
    "## Series Length Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a2c71f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series length distribution:\n",
      "count    2600.0\n",
      "mean      312.0\n",
      "std         0.0\n",
      "min       312.0\n",
      "25%       312.0\n",
      "50%       312.0\n",
      "75%       312.0\n",
      "max       312.0\n",
      "Name: weeks, dtype: float64\n",
      "\n",
      "Series with < 260 weeks: 0\n",
      "Series with >= 260 weeks: 2,600\n"
     ]
    }
   ],
   "source": [
    "# How much history do you have per series?\n",
    "series_length = train_df.groupby('unique_id')['ds'].agg(['min', 'max', 'count'])\n",
    "series_length['weeks'] = (series_length['max'] - series_length['min']).dt.days / 7\n",
    "\n",
    "print(\"Series length distribution:\")\n",
    "print(series_length['weeks'].describe())\n",
    "print(f\"\\nSeries with < 260 weeks: {(series_length['weeks'] < 260).sum():,}\")\n",
    "print(f\"Series with >= 260 weeks: {(series_length['weeks'] >= 260).sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45967cc4",
   "metadata": {},
   "source": [
    "## Diagnosing Time Series Horizon Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48a399a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking data frequency...\n",
      "\n",
      "100_analgesics:\n",
      "ds\n",
      "7.0    312\n",
      "Name: count, dtype: int64\n",
      "\n",
      "100_bath soap:\n",
      "ds\n",
      "7.0    312\n",
      "Name: count, dtype: int64\n",
      "\n",
      "100_bathroom tissues:\n",
      "ds\n",
      "7.0    312\n",
      "Name: count, dtype: int64\n",
      "\n",
      "100_beer:\n",
      "ds\n",
      "7.0    312\n",
      "Name: count, dtype: int64\n",
      "\n",
      "100_bottled juices:\n",
      "ds\n",
      "7.0    312\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Day of week distribution:\n",
      "ds\n",
      "Wednesday    813800\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Checking for gaps...\n"
     ]
    }
   ],
   "source": [
    "# Check 1: Verify your data's actual frequency\n",
    "print(\"Checking data frequency...\")\n",
    "for uid in train_df['unique_id'].unique()[:5]:  # Check first 5 series\n",
    "    ts = train_df[train_df['unique_id'] == uid].sort_values('ds')\n",
    "    date_diffs = ts['ds'].diff().dt.days.value_counts()\n",
    "    print(f\"\\n{uid}:\")\n",
    "    print(date_diffs.head())\n",
    "\n",
    "# Check 2: What day of week are your dates?\n",
    "print(\"\\nDay of week distribution:\")\n",
    "print(train_df['ds'].dt.day_name().value_counts())\n",
    "\n",
    "# Check 3: Are there gaps?\n",
    "print(\"\\nChecking for gaps...\")\n",
    "for uid in train_df['unique_id'].unique()[:10]:\n",
    "    ts = train_df[train_df['unique_id'] == uid].sort_values('ds')\n",
    "    expected_periods = (ts['ds'].max() - ts['ds'].min()).days // 7 + 1\n",
    "    actual_periods = len(ts)\n",
    "    if expected_periods != actual_periods:\n",
    "        print(f\"{uid}: expected {expected_periods}, got {actual_periods}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0fddba",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c29d7c3",
   "metadata": {},
   "source": [
    "## Distribution of Revenue by Store and Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebe3bfb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Analyzing Level: STORE_CATEGORY ---\n",
      "--- Analyzing Level: STORE ---\n",
      "--- Analyzing Level: CATEGORY ---\n",
      "--- Analyzing Level: OVERALL ---\n",
      "\n",
      "==================================================================================\n",
      "REVENUE DISTRIBUTION SUMMARY BY HIERARCHY LEVEL\n",
      "==================================================================================\n",
      "                n_unique_series  total_observations  sparsity_percent  avg_cv  \\\n",
      "level                                                                           \n",
      "STORE_CATEGORY             2600              813800             10.71     NaN   \n",
      "STORE                        93              813800             10.71     NaN   \n",
      "CATEGORY                     28              813800             10.71    0.61   \n",
      "OVERALL                    2600              813800             10.71    1.36   \n",
      "\n",
      "                mean_revenue  median_revenue  std_revenue  p90_revenue  \\\n",
      "level                                                                    \n",
      "STORE_CATEGORY       7550.86         7386.90      2619.32     10402.15   \n",
      "STORE                7550.46         4333.25      9135.09     18388.65   \n",
      "CATEGORY             7555.80         7294.83      4567.49     12863.70   \n",
      "OVERALL              7550.86         4305.72     10241.66     18277.25   \n",
      "\n",
      "                p95_revenue  \n",
      "level                        \n",
      "STORE_CATEGORY     11646.23  \n",
      "STORE              25677.94  \n",
      "CATEGORY           15174.39  \n",
      "OVERALL            27262.38  \n"
     ]
    }
   ],
   "source": [
    "# Assuming df_panel is your dense DataFrame with columns:\n",
    "# ['store', 'category', 'y'] where 'y' is the revenue (0-filled)\n",
    "\n",
    "from importlib.metadata import distribution\n",
    "\n",
    "\n",
    "def analyze_revenue_distribution(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Performs a deep distribution analysis of the revenue (y) across all\n",
    "    specified hierarchy levels.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split the unique_id into store and category\n",
    "    df['store'] = df['unique_id'].apply(lambda s: s.split('_')[0])\n",
    "    df['category'] = df['unique_id'].apply(lambda s: s.split('_')[1])\n",
    "    \n",
    "    # Define the hierarchy levels to analyze\n",
    "    levels = [\n",
    "        ['store', 'category'],  # STORE_CATEGORY (bottom level)\n",
    "        ['store'],              # STORE\n",
    "        ['category'],           # CATEGORY\n",
    "        []                      # Overall (All Hierarchy Levels)\n",
    "    ]\n",
    "    \n",
    "    analysis_results = []\n",
    "\n",
    "    for level_cols in levels:\n",
    "        \n",
    "        # Determine the name for the current level\n",
    "        if not level_cols:\n",
    "            level_name = \"OVERALL\"\n",
    "        else:\n",
    "            level_name = \"_\".join(col.upper() for col in level_cols)\n",
    "            \n",
    "        print(f\"--- Analyzing Level: {level_name} ---\")\n",
    "\n",
    "        # 1. Aggregate the Data\n",
    "        if level_cols:\n",
    "            # Group by the current level and calculate descriptive stats for revenue ('y')\n",
    "            grouped_stats = df.groupby(level_cols)['y'].agg(\n",
    "                mean_revenue='mean',\n",
    "                median_revenue='median',\n",
    "                std_revenue='std',\n",
    "                min_revenue='min',\n",
    "                max_revenue='max',\n",
    "                p90_revenue=lambda x: x.quantile(0.90),\n",
    "                p95_revenue=lambda x: x.quantile(0.95),\n",
    "                total_series='size', # Number of time-steps for this group\n",
    "                true_zeros=lambda x: (x == 0).sum() # Count of weeks with zero sales\n",
    "            ).reset_index()\n",
    "            \n",
    "            # Now, calculate the number of unique series in this level\n",
    "            n_unique_series = grouped_stats.shape[0]\n",
    "            \n",
    "            # Calculate the average of the metrics across all series in this level\n",
    "            level_summary = grouped_stats[[\n",
    "                'mean_revenue', 'median_revenue', 'std_revenue', \n",
    "                'p90_revenue', 'p95_revenue', 'total_series', 'true_zeros'\n",
    "            ]].mean().to_dict()\n",
    "            \n",
    "            # Add the unique series count\n",
    "            level_summary['n_unique_series'] = n_unique_series\n",
    "            \n",
    "            # Calculate total non-zero observations for sparsity check\n",
    "            level_summary['total_observations'] = grouped_stats['total_series'].sum()\n",
    "            level_summary['total_zeros'] = grouped_stats['true_zeros'].sum()\n",
    "            \n",
    "        else:\n",
    "            # For OVERALL level, use the entire 'y' column\n",
    "            level_summary = {\n",
    "                'n_unique_series': df.groupby(['store', 'category']).ngroups, # Total bottom-level series\n",
    "                'mean_revenue': df['y'].mean(),\n",
    "                'median_revenue': df['y'].median(),\n",
    "                'std_revenue': df['y'].std(),\n",
    "                'min_revenue': df['y'].min(),\n",
    "                'max_revenue': df['y'].max(),\n",
    "                'p90_revenue': df['y'].quantile(0.90),\n",
    "                'p95_revenue': df['y'].quantile(0.95),\n",
    "                'total_observations': len(df),\n",
    "                'total_zeros': (df['y'] == 0).sum(),\n",
    "            }\n",
    "        \n",
    "        # 2. Calculate Derived Metrics (Sparsity, Volatility)\n",
    "        level_summary['level'] = level_name\n",
    "        \n",
    "        # Sparsity: Percentage of zero-sale weeks\n",
    "        level_summary['sparsity_percent'] = (level_summary['total_zeros'] / level_summary['total_observations']) * 100\n",
    "        \n",
    "        # Coefficient of Variation (CV) as a measure of relative volatility\n",
    "        # Note: Using the mean of std_revenue divided by the mean of mean_revenue for grouped levels\n",
    "        if level_name != \"OVERALL\" and level_summary['mean_revenue'] != 0:\n",
    "            # Calculate average CV for the groups\n",
    "            grouped_stats['cv'] = grouped_stats['std_revenue'] / grouped_stats['mean_revenue']\n",
    "            level_summary['avg_cv'] = grouped_stats['cv'].mean()\n",
    "        elif level_name == \"OVERALL\" and level_summary['mean_revenue'] != 0:\n",
    "             level_summary['avg_cv'] = level_summary['std_revenue'] / level_summary['mean_revenue']\n",
    "        else:\n",
    "             level_summary['avg_cv'] = np.nan\n",
    "        \n",
    "        analysis_results.append(level_summary)\n",
    "\n",
    "    # Convert results list to a final summary DataFrame\n",
    "    summary_df = pd.DataFrame(analysis_results).set_index('level')\n",
    "    \n",
    "    # Select and format final columns for clean output\n",
    "    final_cols = [\n",
    "        'n_unique_series', 'total_observations', 'sparsity_percent', 'avg_cv',\n",
    "        'mean_revenue', 'median_revenue', 'std_revenue', 'p90_revenue', 'p95_revenue'\n",
    "    ]\n",
    "    \n",
    "    # Ensure all columns exist, handle missing ones (like when level is OVERALL)\n",
    "    for col in final_cols:\n",
    "        if col not in summary_df.columns:\n",
    "            summary_df[col] = np.nan\n",
    "            \n",
    "    return summary_df[final_cols].round(2)\n",
    "\n",
    "\n",
    "\n",
    "distribution_summary = analyze_revenue_distribution(train_df)\n",
    "print(\"\\n==================================================================================\")\n",
    "print(\"REVENUE DISTRIBUTION SUMMARY BY HIERARCHY LEVEL\")\n",
    "print(\"==================================================================================\")\n",
    "print(distribution_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "valuation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
