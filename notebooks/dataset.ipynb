{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d975e610",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69b614f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from valuation.config import SALES_DATA_FILEPATH, CATEGORY_DATA_FILEPATH, STORE_DATA_FILEPATH, TRAIN_DATA_FILEPATH, VALIDATION_DATA_FILEPATH, TEST_DATA_FILEPATH\n",
    "from valuation.utils.io import IOService\n",
    "from valuation.utils.data import DataFrameSplitter\n",
    "from valuation.utils.print import Printer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca45de19",
   "metadata": {},
   "source": [
    "# Dominick's Finer Foods Sales Dataset\n",
    "\n",
    "The Dominick's Finer Foods (DFF) dataset, provided by the James M. Kilts Center at the University of Chicago Booth School of Business, comprises sales transactions across 28 distinct categorize from the 100-store retail chain. Spanning nearly eight years from September 1989 to May 1997, these data constitute transactions at the Uniform Product Code (UPC) level. \n",
    "\n",
    "***\n",
    "\n",
    "## Key Data Fields\n",
    "\n",
    "* **STORE**: A unique numeric identifier for each retail filepath.\n",
    "* **UPC**: The Uniform Product Code, a unique identifier for each distinct product.\n",
    "* **WEEK**: A numeric value representing the week of the transaction.\n",
    "* **MOVE**: The total number of individual units sold for a given UPC in a specific store and week.\n",
    "* **QTY**: The number of items in a promotional bundle (e.g., `3` for a 3-pack). This value is `1` for individually sold items.\n",
    "* **PRICE**: The shelf price for the bundle or individual item. Revenue is calculated as **`(PRICE * MOVE) / QTY`** to account for bundled sales.\n",
    "* **PROFIT**: The **gross margin percentage** for the product. A value of `25.3` corresponds to a 25.3% margin.\n",
    "* **SALE**: A flag indicating a promotion ('B', 'C', 'S'). This flag is noted by the data providers as being inconsistently applied.\n",
    "* **OK**: A data quality flag where a value of `1` indicates the record is considered valid for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456fd983",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "The data preprocessing stage transforms the raw, transaction-level records into a series of analysis-ready datasets. These files serve distinct purposes, from model training to strategic performance analysis.\n",
    "\n",
    "***\n",
    "\n",
    "### Sales Dataset\n",
    "* **`sales_data.csv`**: Contains cleaned sales data aggregated weekly by store and category. Revenue and gross profit are calculated for each record and the week start and end dates are added.\n",
    "\n",
    "### Modeling Datasets\n",
    "The primary dataset is partitioned chronologically for model development.\n",
    "\n",
    "* **`train.csv`**: The **Training Set** contains the first 280 weeks (~70%) of the data, and are used to train forecasting models. All performance analysis is derived from this subset.\n",
    "* **`validation.csv`**: The **Validation Set** comprises the next 60 weeks of data (~15%) and are used for hyperparameter tuning.\n",
    "* **`test.csv`**: The **Test Set is the hold-test set containing the final 60 weeks of data. This dataset is set aside for unbiased model evaluation.\n",
    "\n",
    "### Performance Analysis Datasets\n",
    "Summary datasets are derived from the training set to support strategic analysis.\n",
    "\n",
    "* **`same_store_sales_growth.csv`**: Contains the aggregated year-over-year Same-Store Sales (SSS) growth for the company.\n",
    "* **`store_performance.csv`**: Details store-level metrics from the final year of the training data, including year-over-year sales growth, total gross profit, and gross margin percentage.\n",
    "* **`category_performance.csv`**: Details category-level metrics, structured identically to the store performance file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8bc0b1",
   "metadata": {},
   "source": [
    "## Sales Dataset\n",
    "The 28 category-level files are now processed and concatenated into a single, aggregated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceea485e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing category: Bathroom Tissues from file: wtti.zip: 100%|██████████| 28/28 [08:33<00:00, 18.32s/it]  \n"
     ]
    }
   ],
   "source": [
    "# Obtain categories and filenames from config\n",
    "from valuation.config import CONFIG_CATEGORY_FILEPATH, ConfigReader\n",
    "from valuation.prep import SalesDataPrep\n",
    "\n",
    "# Instantiate the config reader and read the category filenames\n",
    "config_reader = ConfigReader()\n",
    "category_filenames = config_reader.read(CONFIG_CATEGORY_FILEPATH)\n",
    "\n",
    "# Instantiate the sales data processor\n",
    "processor = SalesDataPrep()\n",
    "\n",
    "# Run the processor pipeline\n",
    "processor.prepare(category_filenames=category_filenames, force=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46d0192",
   "metadata": {},
   "source": [
    "## Modeling Datasets\n",
    "For model development, the preprocessed dataset is partitioned chronologically into training, validation, and test subsets. The split is based on a 70/15/15 division of the total weeks in the dataset, which simulates a real-world forecasting scenario by training on past data to predict future outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac4118e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "                             Dataset Split Metadata                             \n",
      "                                 n_train | 559719\n",
      "                            n_validation | 123470\n",
      "                                  n_test | 134067\n",
      "                                 n_total | 817256\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Instantiate Services\n",
    "partitioner = DataFrameSplitter()\n",
    "printer = Printer()\n",
    "\n",
    "# Load Data\n",
    "sales_data = IOService.read(SALES_DATA_FILEPATH)\n",
    "\n",
    "# Split Data\n",
    "splits = partitioner.split_by_proportion_of_values(df=sales_data, val_col='week', train_size=0.7, val_size=0.15)\n",
    "\n",
    "# Save Split Data\n",
    "IOService.write(data=splits['data'].get(\"train\",None), filepath=TRAIN_DATA_FILEPATH)\n",
    "IOService.write(data=splits['data'].get(\"validation\",None), filepath=VALIDATION_DATA_FILEPATH)\n",
    "IOService.write(data=splits['data'].get(\"test\",None), filepath=TEST_DATA_FILEPATH)\n",
    "\n",
    "# Print Split Metadata\n",
    "printer.print_dict(data=splits['meta'], title=\"Dataset Split Metadata\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "valuation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
