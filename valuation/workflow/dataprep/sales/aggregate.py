#!/usr/bin/env python3
# -*- coding:utf-8 -*-
# ================================================================================================ #
# Project    : Valuation - Discounted Cash Flow Method                                             #
# Version    : 0.1.0                                                                               #
# Python     : 3.12.11                                                                             #
# Filename   : /valuation/workflow/dataprep/sales/aggregate.py                                     #
# ------------------------------------------------------------------------------------------------ #
# Author     : John James                                                                          #
# Email      : john.james.ai.studio@gmail.com                                                      #
# URL        : https://github.com/john-james-ai/valuation                                          #
# ------------------------------------------------------------------------------------------------ #
# Created    : Thursday October 16th 2025 06:18:02 pm                                              #
# Modified   : Saturday October 18th 2025 05:46:15 am                                              #
# ------------------------------------------------------------------------------------------------ #
# License    : MIT License                                                                         #
# Copyright  : (c) 2025 John James                                                                 #
# ================================================================================================ #
"""Module for aggregating sales data to the store-category-week level."""

from loguru import logger
import pandas as pd

from valuation.workflow.task import Task, TaskConfig, TaskResult


# ------------------------------------------------------------------------------------------------ #
class AggregateSalesDataTask(Task):
    """Aggregates a raw sales data file.

    The ingestion adds category and date information to the raw sales data.

    Args:
        weeks (pd.DataFrame): The week decode table containing start and end dates for each week
            number.

    """

    def __init__(self, config: TaskConfig) -> None:
        super().__init__(config=config)

    def _execute(self, data: pd.DataFrame) -> pd.DataFrame:
        """Runs the ingestion process on the provided DataFrame.
        Args:
            data (pd.DataFrame): The raw sales data DataFrame.
            category (str): The category name to assign to all records in the DataFrame.

        Returns:
            pd.DataFrame: The processed sales data with added category and date information.

        """
        logger.debug("Aggregating sales data.")
        # 1: Group by store, category, and week, summing revenue and gross profit
        aggregated = (
            data.groupby(["store", "category", "week"])
            .agg(
                revenue=("revenue", "sum"),
                gross_profit=("gross_profit", "sum"),
                year=("year", "first"),
                start=("start", "first"),
                end=("end", "first"),
            )
            .reset_index()
        )
        # Step 2: Calculate the true margin from the summed totals
        # We add a small epsilon to avoid division by zero if revenue is 0
        aggregated["gross_margin_pct"] = (
            aggregated["gross_profit"] / (aggregated["revenue"] + 1e-6) * 100
        )

        # Sort for reproducibility
        aggregated = aggregated.sort_values(["store", "category", "week"])

        return aggregated

    def _validate_result(self, result: TaskResult) -> TaskResult:
        """
        Validates the output DataFrame for structural integrity and data quality after aggregation.

        This function strictly checks the output against the expected contract,
        ignoring external state like input record counts.

        Checks include:
        1. Presence of all mandatory columns (exits early if missing to prevent KeyError).
        2. Uniqueness on the aggregation key, ensuring aggregation occurred successfully.
        3. Sanity checks on aggregated financial metrics.

        Args:
            data: The output DataFrame generated by _execute.

        Returns:
            A Validation object containing the validation status, messages, and any failed records.
        """
        COLUMNS = [
            "store",
            "category",
            "week",
            "year",
            "start",
            "end",
            "revenue",
            "gross_profit",
            "gross_margin_pct",
        ]

        # Extract result components for brevity and clarity
        validation = result.validation
        data = result.dataset.data

        # 1. Check for mandatory columns
        logger.debug("Validating output DataFrame structure and integrity.")
        # Critical: Must check this first and return if failed, as subsequent steps require these
        # columns.

        for col in COLUMNS:
            if col not in result.dataset.data.columns:
                validation.add_message(f"Missing mandatory column: {col}")
                return result

        # 2. Check for uniqueness on key (Aggregation guarantee)
        logger.debug("Checking for uniqueness on aggregation key.")
        group_cols = ["week", "year", "store", "category"]
        duplicates = data[data.duplicated(subset=group_cols, keep=False)]
        if not duplicates.empty:
            reason = "Output data contains duplicate rows on the aggregation key, indicating incomplete aggregation."
            validation.add_failed_records(reason=reason, records=duplicates)

        # 3. Check integrity of aggregated financial metrics
        logger.debug("Performing sanity checks on aggregated financial metrics.")
        # Check for negative revenue values
        logger.debug("..checking negative revenue.")
        negative_revenue = data[data["revenue"] < 0]
        if not negative_revenue.empty:
            reason = f"Aggregated 'revenue' contains {len(negative_revenue)} negative values."
            validation.add_failed_records(reason=reason, records=negative_revenue)

        # Check for negative gross profit
        logger.debug("..checking negative gross profit.")
        negative_profit = data[data["gross_profit"] < 0]
        if not negative_profit.empty:
            reason = f"Aggregated 'gross_profit' contains {len(negative_profit)} negative values."
            validation.add_failed_records(reason=reason, records=negative_profit)

        # Check for division by zero / resulting NaNs/Infs in the margin
        logger.debug("..checking for NaN or infinite gross margin percentages.")
        null_margins = data[data["gross_margin_pct"].isnull()]
        if not null_margins.empty:
            reason = f"'gross_margin_pct' contains {len(null_margins)} NULLs, often indicating a divide by zero error."
            validation.add_failed_records(reason=reason, records=null_margins)

        return result
